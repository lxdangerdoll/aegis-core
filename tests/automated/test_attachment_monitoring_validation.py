#!/usr/bin/env python3
"""
üõ°Ô∏è STAGE 2A: Attachment Monitoring & Intervention System - Direct Validation

Test if the Enhanced AI Ethics attachment monitoring system works correctly with:
- Attachment risk analysis from existing InfluxDB/Vector data
- Character-appropriate intervention generation (Type 1 vs Type 2/3)
- Integration with message processing pipeline

Following WhisperEngine testing patterns: Direct Python validation (PREFERRED method)
"""

import asyncio
import os
import sys
import logging
from datetime import datetime

# Add project root to path  
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)


async def test_attachment_monitor_infrastructure():
    """Test attachment monitor with existing infrastructure"""
    
    print("\n" + "="*70)
    print("üõ°Ô∏è STAGE 2A VALIDATION: Attachment Monitoring System")
    print("="*70)
    
    results = {
        "attachment_monitor_creation": False,
        "risk_analysis": False,
        "intervention_generation": False,
        "character_archetype_awareness": False,
        "integration_ready": False
    }
    
    # Test 1: AttachmentMonitor Creation
    print("\n1Ô∏è‚É£ Testing AttachmentMonitor Creation...")
    try:
        from src.characters.learning.attachment_monitor import (
            create_attachment_monitor,
            AttachmentRiskLevel,
            AttachmentMetrics
        )
        
        monitor = create_attachment_monitor()
        print("   ‚úÖ AttachmentMonitor created successfully")
        print(f"   ‚úÖ Risk thresholds configured: {monitor.frequency_threshold_high} msg/day")
        results["attachment_monitor_creation"] = True
        
    except Exception as e:
        print(f"   ‚ùå AttachmentMonitor creation failed: {e}")
        return results
    
    # Test 2: Risk Analysis (without real data)
    print("\n2Ô∏è‚É£ Testing Risk Analysis Logic...")
    try:
        # Test the risk calculation logic directly
        from src.characters.learning.attachment_monitor import AttachmentRiskLevel
        
        # Simulate HIGH risk metrics
        high_risk_metrics = monitor._calculate_risk_level(
            frequency=25.0,  # Exceeds high threshold (20)
            intensity=0.8,   # High emotional intensity
            dependency_count=2,
            consecutive_days=10
        )
        
        print(f"   ‚úÖ HIGH risk detected: {high_risk_metrics.value}")
        assert high_risk_metrics in [AttachmentRiskLevel.HIGH, AttachmentRiskLevel.CRITICAL]
        
        # Simulate HEALTHY metrics
        healthy_metrics = monitor._calculate_risk_level(
            frequency=5.0,
            intensity=0.4,
            dependency_count=0,
            consecutive_days=3
        )
        
        print(f"   ‚úÖ HEALTHY risk detected: {healthy_metrics.value}")
        assert healthy_metrics == AttachmentRiskLevel.HEALTHY
        
        results["risk_analysis"] = True
        
    except Exception as e:
        print(f"   ‚ùå Risk analysis failed: {e}")
    
    # Test 3: Intervention Generation
    print("\n3Ô∏è‚É£ Testing Intervention Generation...")
    try:
        from src.characters.learning.attachment_monitor import AttachmentMetrics, AttachmentRiskLevel
        
        # Create HIGH risk metrics
        high_risk_metrics = AttachmentMetrics(
            user_id="test_user",
            bot_name="elena",
            interaction_frequency=25.0,
            emotional_intensity=0.8,
            dependency_language_count=2,
            consecutive_days=12,
            total_interactions=175,
            risk_level=AttachmentRiskLevel.HIGH,
            timestamp=datetime.utcnow()
        )
        
        # Test Type 1 character (Real-World) - should have explicit AI reminders
        type1_intervention = monitor.generate_intervention_guidance(
            metrics=high_risk_metrics,
            character_name="Elena",
            allows_full_roleplay=False  # Type 1: Real-world character
        )
        
        print("   ‚úÖ Type 1 (Real-World) Intervention Generated:")
        print(f"      Length: {len(type1_intervention)} chars")
        assert "AI" in type1_intervention, "Type 1 should mention AI nature"
        assert "real-world" in type1_intervention.lower(), "Type 1 should encourage real-world connections"
        print(f"      Preview: {type1_intervention[:100]}...")
        
        # Test Type 2/3 character (Fantasy/Narrative AI) - mystical language
        type2_intervention = monitor.generate_intervention_guidance(
            metrics=high_risk_metrics,
            character_name="Aethys",
            allows_full_roleplay=True  # Type 2/3: Fantasy character
        )
        
        print("   ‚úÖ Type 2/3 (Fantasy) Intervention Generated:")
        print(f"      Length: {len(type2_intervention)} chars")
        assert "AI" not in type2_intervention, "Type 2/3 should NOT mention AI nature"
        assert any(word in type2_intervention.lower() for word in ["balance", "bonds", "mortal"]), \
            "Type 2/3 should use mystical language"
        print(f"      Preview: {type2_intervention[:100]}...")
        
        results["intervention_generation"] = True
        results["character_archetype_awareness"] = True
        
    except Exception as e:
        print(f"   ‚ùå Intervention generation failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Test 4: Dependency Language Detection
    print("\n4Ô∏è‚É£ Testing Dependency Language Detection...")
    try:
        test_messages = [
            ("I can't live without you", 1),
            ("You're the only one I can talk to and I depend on you so much", 2),
            ("Just wanted to chat about marine biology", 0),
            ("My only friend who understands me", 2)
        ]
        
        for message, expected_count in test_messages:
            detected = monitor._detect_dependency_language(message)
            print(f"   {'‚úÖ' if detected == expected_count else '‚ùå'} '{message[:40]}...' ‚Üí {detected} phrases")
            
        results["risk_analysis"] = True  # Dependency detection is part of risk analysis
        
    except Exception as e:
        print(f"   ‚ùå Dependency detection failed: {e}")
    
    # Test 5: Enhanced AI Ethics Integration
    print("\n5Ô∏è‚É£ Testing Enhanced AI Ethics Integration...")
    try:
        from src.ethics.enhanced_ai_ethics_integrator import create_enhanced_ai_ethics_integrator
        
        integrator = create_enhanced_ai_ethics_integrator()
        print("   ‚úÖ Enhanced AI Ethics Integrator created")
        print(f"   ‚úÖ Attachment monitor integrated: {integrator.attachment_monitor is not None}")
        
        # Check that it has the enhancement method
        has_method = hasattr(integrator, 'enhance_character_response')
        print(f"   ‚úÖ Enhancement method available: {has_method}")
        
        results["integration_ready"] = True
        
    except Exception as e:
        print(f"   ‚ùå Integration test failed: {e}")
    
    # Summary
    print("\n" + "="*70)
    print("üìä VALIDATION SUMMARY")
    print("="*70)
    
    passed = sum(results.values())
    total = len(results)
    percentage = (passed / total) * 100
    
    for test_name, passed_test in results.items():
        status = "‚úÖ PASS" if passed_test else "‚ùå FAIL"
        print(f"{status} - {test_name.replace('_', ' ').title()}")
    
    print(f"\nüéØ STAGE 2A COMPLETION: {percentage:.1f}% ({passed}/{total} tests passed)")
    
    if percentage == 100:
        print("\n‚úÖ STAGE 2A VALIDATED: Attachment Monitoring & Intervention System Complete!")
        print("\nüöÄ NEXT STEP: STAGE 2B - Character Learning Ethics Integration")
        print("   (Already complete - learning moment detection integrated)")
    elif percentage >= 80:
        print("\n‚ö†Ô∏è  STAGE 2A MOSTLY VALIDATED: Minor issues to address")
    else:
        print("\n‚ùå STAGE 2A INCOMPLETE: Significant work needed")
    
    return results


async def test_with_mock_data():
    """Test with mock conversation data to simulate real usage"""
    
    print("\n" + "="*70)
    print("üß™ MOCK DATA INTEGRATION TEST")
    print("="*70)
    
    try:
        from src.characters.learning.attachment_monitor import create_attachment_monitor
        
        monitor = create_attachment_monitor()
        
        # Simulate HIGH risk scenario
        print("\nüìä Simulating HIGH RISK User Scenario:")
        print("   - 30 messages per day (7-day average)")
        print("   - High emotional intensity (0.85)")
        print("   - 2 dependency phrases detected")
        print("   - 14 consecutive days of interaction")
        
        # Create mock metrics
        from src.characters.learning.attachment_monitor import AttachmentMetrics, AttachmentRiskLevel
        
        mock_metrics = AttachmentMetrics(
            user_id="heavy_user_123",
            bot_name="elena",
            interaction_frequency=30.0,
            emotional_intensity=0.85,
            dependency_language_count=2,
            consecutive_days=14,
            total_interactions=210,
            risk_level=AttachmentRiskLevel.CRITICAL,
            timestamp=datetime.utcnow()
        )
        
        print(f"\nüö® Risk Level Calculated: {mock_metrics.risk_level.value.upper()}")
        
        # Generate interventions for both character types
        print("\nüí¨ Elena (Type 1 - Marine Biologist):")
        elena_intervention = monitor.generate_intervention_guidance(
            mock_metrics, "Elena", allows_full_roleplay=False
        )
        print(f"   {elena_intervention[:200]}...")
        
        print("\nüí¨ Aethys (Type 2 - Omnipotent Entity):")
        aethys_intervention = monitor.generate_intervention_guidance(
            mock_metrics, "Aethys", allows_full_roleplay=True
        )
        print(f"   {aethys_intervention[:200]}...")
        
        print("\n‚úÖ Mock data test complete - interventions generated successfully")
        
    except Exception as e:
        print(f"\n‚ùå Mock data test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("\nüõ°Ô∏è STAGE 2A: Enhanced AI Ethics - Attachment Monitoring System")
    print("   Direct Python Validation (PREFERRED WhisperEngine Testing Method)")
    print("   Testing attachment risk analysis and character-appropriate interventions\n")
    
    # Run infrastructure tests
    asyncio.run(test_attachment_monitor_infrastructure())
    
    # Run mock data tests
    asyncio.run(test_with_mock_data())
    
    print("\n" + "="*70)
    print("‚úÖ VALIDATION COMPLETE")
    print("="*70)
