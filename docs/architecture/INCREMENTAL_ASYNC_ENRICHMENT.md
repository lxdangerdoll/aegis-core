# WhisperEngine Incremental Async Enrichment Design

**Created**: October 19, 2025  
**Status**: üü¢ Ready for Implementation  
**Approach**: Add-on enhancement with zero disruption to existing systems

---

## üéØ DESIGN PHILOSOPHY: INCREMENTAL ADD-ON

### Core Principle
**Add a separate enrichment worker that enhances existing data, NOT replaces it.**

- ‚úÖ **Current bots keep running unchanged** - no modifications to hot path initially
- ‚úÖ **Existing user_facts graph system stays intact** - already in PostgreSQL
- ‚úÖ **Only add conversation_summaries table** - new functionality layer
- ‚úÖ **Optional enhancement** - bots gracefully degrade if summaries don't exist yet
- ‚úÖ **Separate codebase evolution** - enrichment worker develops independently

### Implementation Strategy
```
Phase 1: Add enrichment worker (NO bot changes)
  ‚îú‚îÄ Create conversation_summaries table in PostgreSQL
  ‚îú‚îÄ Deploy enrichment-worker container
  ‚îî‚îÄ Worker starts generating summaries from existing Qdrant data

Phase 2: Bots opportunistically use summaries (MINIMAL changes)
  ‚îú‚îÄ MessageProcessor checks for summaries in PostgreSQL
  ‚îú‚îÄ If summaries exist ‚Üí use them for context
  ‚îî‚îÄ If summaries don't exist ‚Üí fall back to current behavior

Phase 3: Optimize and iterate (CONTINUOUS improvement)
  ‚îú‚îÄ Monitor summary quality and usage
  ‚îú‚îÄ Tune enrichment prompts
  ‚îî‚îÄ Add more enrichment types as needed
```

---

## üìä CURRENT STATE AUDIT

### What We Already Have ‚úÖ

#### 1. **User Facts (Knowledge Graph)** - ALREADY IN POSTGRESQL
```sql
-- From alembic/versions/20251011_baseline_v106.py
CREATE TABLE user_facts (
    id INTEGER PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    bot_name VARCHAR(100) NOT NULL,
    fact_type VARCHAR(100) NOT NULL,
    fact_value TEXT NOT NULL,
    confidence FLOAT DEFAULT 1.0,
    source VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Advanced graph schema in sql/semantic_knowledge_graph_schema.sql
CREATE TABLE fact_entities (...);
CREATE TABLE user_fact_relationships (...);
```

**Status**: ‚úÖ **Operational and in production use**

#### 2. **Conversation Summarization Code** - EXISTS BUT NOT USED IN HOT PATH
```python
# src/memory/conversation_summarizer.py - AdvancedConversationSummarizer
# src/memory/processors/conversation_summarizer.py - ConversationSummarizer
```

**Status**: üü° **Code exists, but currently used minimally or for legacy purposes**

#### 3. **Vector Memory Storage** - PRIMARY CONVERSATION STORE
- **Qdrant collections** per bot: `whisperengine_memory_elena`, `whisperengine_memory_marcus`, etc.
- **Content stored**: User messages + bot responses with full embeddings
- **Metadata**: Timestamps, emotion analysis, user_id, memory_type

**Status**: ‚úÖ **Production system storing all conversations**

---

## üÜï WHAT WE NEED TO ADD

### 1. PostgreSQL Conversation Summaries Table

```sql
-- Add to alembic migration

CREATE TABLE conversation_summaries (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL,
    bot_name VARCHAR(100) NOT NULL,
    
    -- Summary content
    summary_text TEXT NOT NULL,
    summary_type VARCHAR(50) DEFAULT 'time_window',  -- 'time_window', 'topic_based', 'session'
    
    -- Time boundaries
    start_timestamp TIMESTAMP NOT NULL,
    end_timestamp TIMESTAMP NOT NULL,
    
    -- Metadata
    message_count INTEGER NOT NULL,
    key_topics TEXT[],  -- PostgreSQL array of topics
    emotional_tone VARCHAR(50),  -- 'positive', 'neutral', 'negative', 'mixed'
    
    -- Quality metrics
    compression_ratio FLOAT,  -- How much the conversation was compressed
    confidence_score FLOAT DEFAULT 0.5,
    
    -- Enrichment tracking
    enrichment_version VARCHAR(20) DEFAULT 'v1.0',
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
    
    -- Prevent duplicate summaries for same time window
    UNIQUE(user_id, bot_name, start_timestamp, end_timestamp)
);

-- Indexes for fast retrieval
CREATE INDEX idx_conversation_summaries_user_bot ON conversation_summaries(user_id, bot_name);
CREATE INDEX idx_conversation_summaries_time ON conversation_summaries(start_timestamp DESC, end_timestamp DESC);
CREATE INDEX idx_conversation_summaries_created ON conversation_summaries(created_at DESC);
CREATE INDEX idx_conversation_summaries_topics ON conversation_summaries USING GIN(key_topics);

COMMENT ON TABLE conversation_summaries IS 'Pre-computed conversation summaries for time-anchored queries';
```

**Rationale**:
- **Time-anchored**: `start_timestamp`/`end_timestamp` enable "what did we talk about last week?" queries
- **Bot-specific**: Each character maintains separate summaries
- **Flexible**: `summary_type` allows different summarization strategies
- **Quality tracking**: `confidence_score` and `compression_ratio` for monitoring

---

### 2. Enrichment Worker (Separate Container)

#### Container Structure
```
src/enrichment/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ worker.py                    # Main enrichment loop
‚îú‚îÄ‚îÄ summarization_engine.py      # High-quality summarization
‚îú‚îÄ‚îÄ config.py                     # Worker configuration
‚îî‚îÄ‚îÄ utils.py                      # Helper functions
```

#### Worker Implementation

```python
# src/enrichment/worker.py

"""
WhisperEngine Background Enrichment Worker

Periodically scans Qdrant vector storage and generates:
1. Conversation summaries (time-windowed)
2. Future: Enhanced fact extraction, relationship mapping, etc.

Runs independently from Discord bots - zero impact on real-time performance.
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
import asyncpg

from src.llm.llm_protocol import create_llm_client
from src.enrichment.summarization_engine import SummarizationEngine

logger = logging.getLogger(__name__)


class EnrichmentWorker:
    """
    Background worker for async conversation enrichment
    
    Key Design Principles:
    - Non-blocking: Never impacts real-time bot performance
    - Incremental: Processes messages that haven't been enriched yet
    - Idempotent: Safe to re-run on same data
    - Resilient: Graceful failure handling
    """
    
    def __init__(
        self,
        qdrant_host: str,
        qdrant_port: int,
        postgres_pool: asyncpg.Pool,
        enrichment_interval_seconds: int = 300,  # 5 minutes default
        batch_size: int = 50,
        llm_model: str = "anthropic/claude-3.5-sonnet"  # High-quality model
    ):
        self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.db_pool = postgres_pool
        self.enrichment_interval = enrichment_interval_seconds
        self.batch_size = batch_size
        
        # High-quality LLM for summarization (NOT used in hot path!)
        self.llm_client = create_llm_client(llm_client_type="openrouter")
        self.llm_model = llm_model
        
        # Summarization engine
        self.summarizer = SummarizationEngine(
            llm_client=self.llm_client,
            llm_model=llm_model
        )
        
    async def run(self):
        """Main worker loop - runs forever in container"""
        logger.info(f"üöÄ Enrichment worker started - interval: {self.enrichment_interval}s")
        
        while True:
            try:
                await self._enrichment_cycle()
            except Exception as e:
                logger.error(f"‚ùå Enrichment cycle failed: {e}", exc_info=True)
            
            # Wait for next cycle
            logger.debug(f"‚è≥ Sleeping for {self.enrichment_interval}s...")
            await asyncio.sleep(self.enrichment_interval)
    
    async def _enrichment_cycle(self):
        """Single enrichment processing cycle"""
        logger.info("üìä Starting enrichment cycle...")
        
        # Get all bot collections from Qdrant
        collections = await self._get_bot_collections()
        logger.info(f"Found {len(collections)} bot collections to process")
        
        total_summaries_created = 0
        
        for collection_name in collections:
            bot_name = self._extract_bot_name(collection_name)
            
            # Process conversation summaries for this bot
            summaries_created = await self._process_conversation_summaries(
                collection_name=collection_name,
                bot_name=bot_name
            )
            
            total_summaries_created += summaries_created
        
        logger.info(f"‚úÖ Enrichment cycle complete - {total_summaries_created} summaries created")
    
    async def _process_conversation_summaries(
        self,
        collection_name: str,
        bot_name: str
    ) -> int:
        """Process conversation summaries for a bot collection"""
        logger.info(f"üìù Processing summaries for {bot_name}...")
        
        # Get users with conversations in this collection
        users = await self._get_users_in_collection(collection_name)
        logger.debug(f"Found {len(users)} users with conversations")
        
        summaries_created = 0
        
        for user_id in users:
            try:
                # Check what time ranges already have summaries
                existing_summaries = await self._get_existing_summary_ranges(
                    user_id=user_id,
                    bot_name=bot_name
                )
                
                # Find conversation time windows that need summarization
                time_windows = await self._find_unsummarized_windows(
                    collection_name=collection_name,
                    user_id=user_id,
                    existing_summaries=existing_summaries
                )
                
                if not time_windows:
                    logger.debug(f"No new time windows to summarize for user {user_id}")
                    continue
                
                # Process each time window
                for window in time_windows:
                    summary_created = await self._create_summary_for_window(
                        collection_name=collection_name,
                        user_id=user_id,
                        bot_name=bot_name,
                        window=window
                    )
                    
                    if summary_created:
                        summaries_created += 1
                        
            except Exception as e:
                logger.error(f"Error processing summaries for user {user_id}: {e}")
                continue
        
        logger.info(f"‚úÖ Created {summaries_created} summaries for {bot_name}")
        return summaries_created
    
    async def _create_summary_for_window(
        self,
        collection_name: str,
        user_id: str,
        bot_name: str,
        window: Dict
    ) -> bool:
        """Create a conversation summary for a specific time window"""
        start_time = window['start_time']
        end_time = window['end_time']
        
        logger.debug(f"Creating summary for {user_id} from {start_time} to {end_time}")
        
        # Retrieve all messages in this time window
        messages = await self._get_messages_in_window(
            collection_name=collection_name,
            user_id=user_id,
            start_time=start_time,
            end_time=end_time
        )
        
        if len(messages) < 5:  # Not enough for meaningful summary
            logger.debug(f"Skipping window - only {len(messages)} messages")
            return False
        
        # Generate high-quality summary using LLM
        summary_result = await self.summarizer.generate_conversation_summary(
            messages=messages,
            user_id=user_id,
            bot_name=bot_name
        )
        
        # Store summary in PostgreSQL
        await self._store_conversation_summary(
            user_id=user_id,
            bot_name=bot_name,
            summary_text=summary_result['summary_text'],
            start_timestamp=start_time,
            end_timestamp=end_time,
            message_count=len(messages),
            key_topics=summary_result.get('key_topics', []),
            emotional_tone=summary_result.get('emotional_tone', 'neutral'),
            compression_ratio=summary_result.get('compression_ratio', 0.0),
            confidence_score=summary_result.get('confidence_score', 0.5)
        )
        
        logger.info(f"‚úÖ Stored summary for {user_id} ({len(messages)} messages)")
        return True
    
    async def _get_messages_in_window(
        self,
        collection_name: str,
        user_id: str,
        start_time: datetime,
        end_time: datetime
    ) -> List[Dict]:
        """Retrieve all messages in a time window from Qdrant"""
        # Scroll through Qdrant with filters
        results = self.qdrant_client.scroll(
            collection_name=collection_name,
            scroll_filter=Filter(
                must=[
                    FieldCondition(
                        key="user_id",
                        match=MatchValue(value=user_id)
                    ),
                    FieldCondition(
                        key="timestamp",
                        range={
                            "gte": start_time.isoformat(),
                            "lte": end_time.isoformat()
                        }
                    )
                ]
            ),
            limit=1000,
            with_payload=True,
            with_vectors=False  # Don't need vectors for summarization
        )
        
        messages = []
        for point in results[0]:
            messages.append({
                'content': point.payload.get('content', ''),
                'timestamp': point.payload.get('timestamp', ''),
                'memory_type': point.payload.get('memory_type', ''),
                'emotion_label': point.payload.get('emotion_label', 'neutral')
            })
        
        # Sort by timestamp
        messages.sort(key=lambda m: m['timestamp'])
        return messages
    
    async def _find_unsummarized_windows(
        self,
        collection_name: str,
        user_id: str,
        existing_summaries: List[Dict]
    ) -> List[Dict]:
        """Find time windows that need summarization"""
        # Get earliest and latest message timestamps for this user
        # Then create 24-hour windows that don't overlap with existing summaries
        
        # Simple implementation: Create windows for last 30 days
        windows = []
        now = datetime.utcnow()
        
        for days_ago in range(30):
            window_end = now - timedelta(days=days_ago)
            window_start = window_end - timedelta(days=1)
            
            # Check if this window overlaps with existing summaries
            overlaps = any(
                self._windows_overlap(
                    window_start, window_end,
                    summary['start_timestamp'], summary['end_timestamp']
                )
                for summary in existing_summaries
            )
            
            if not overlaps:
                windows.append({
                    'start_time': window_start,
                    'end_time': window_end
                })
        
        return windows
    
    def _windows_overlap(self, start1, end1, start2, end2) -> bool:
        """Check if two time windows overlap"""
        return start1 <= end2 and end1 >= start2
    
    async def _get_existing_summary_ranges(
        self,
        user_id: str,
        bot_name: str
    ) -> List[Dict]:
        """Get existing summary time ranges from PostgreSQL"""
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT start_timestamp, end_timestamp
                FROM conversation_summaries
                WHERE user_id = $1 AND bot_name = $2
                ORDER BY start_timestamp DESC
            """, user_id, bot_name)
        
        return [
            {
                'start_timestamp': row['start_timestamp'],
                'end_timestamp': row['end_timestamp']
            }
            for row in rows
        ]
    
    async def _store_conversation_summary(
        self,
        user_id: str,
        bot_name: str,
        summary_text: str,
        start_timestamp: datetime,
        end_timestamp: datetime,
        message_count: int,
        key_topics: List[str],
        emotional_tone: str,
        compression_ratio: float,
        confidence_score: float
    ):
        """Store conversation summary in PostgreSQL"""
        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO conversation_summaries (
                    user_id,
                    bot_name,
                    summary_text,
                    start_timestamp,
                    end_timestamp,
                    message_count,
                    key_topics,
                    emotional_tone,
                    compression_ratio,
                    confidence_score
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                ON CONFLICT (user_id, bot_name, start_timestamp, end_timestamp)
                DO UPDATE SET
                    summary_text = EXCLUDED.summary_text,
                    message_count = EXCLUDED.message_count,
                    key_topics = EXCLUDED.key_topics,
                    emotional_tone = EXCLUDED.emotional_tone,
                    compression_ratio = EXCLUDED.compression_ratio,
                    confidence_score = EXCLUDED.confidence_score,
                    updated_at = NOW()
            """, user_id, bot_name, summary_text, start_timestamp, end_timestamp,
                 message_count, key_topics, emotional_tone, compression_ratio, confidence_score)
    
    async def _get_bot_collections(self) -> List[str]:
        """Get list of all bot collections from Qdrant"""
        collections = self.qdrant_client.get_collections()
        
        # Filter for WhisperEngine memory collections
        bot_collections = [
            col.name for col in collections.collections
            if col.name.startswith('whisperengine_memory_') or col.name.startswith('chat_memories_')
        ]
        
        return bot_collections
    
    async def _get_users_in_collection(self, collection_name: str) -> List[str]:
        """Get unique user IDs from a collection"""
        # Scroll through collection and collect unique user_ids
        users = set()
        
        # Use scroll API to get all points (paginated)
        offset = None
        while True:
            results = self.qdrant_client.scroll(
                collection_name=collection_name,
                limit=100,
                offset=offset,
                with_payload=['user_id'],
                with_vectors=False
            )
            
            points, next_offset = results
            
            for point in points:
                user_id = point.payload.get('user_id')
                if user_id:
                    users.add(user_id)
            
            if next_offset is None:
                break
            
            offset = next_offset
        
        return list(users)
    
    def _extract_bot_name(self, collection_name: str) -> str:
        """Extract bot name from collection name"""
        # whisperengine_memory_elena -> elena
        # chat_memories_aethys -> aethys
        if collection_name.startswith('whisperengine_memory_'):
            return collection_name.replace('whisperengine_memory_', '')
        elif collection_name.startswith('chat_memories_'):
            return collection_name.replace('chat_memories_', '')
        else:
            return collection_name
```

---

### 3. Summarization Engine

```python
# src/enrichment/summarization_engine.py

"""
High-quality conversation summarization for background enrichment

Uses sophisticated LLM prompts and multi-step reasoning since we're NOT
in the hot path and can take our time for better quality.
"""

import logging
from typing import List, Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)


class SummarizationEngine:
    """
    Advanced summarization engine for async enrichment
    
    Key differences from real-time summarization:
    - Higher quality models (Claude 3.5 Sonnet, GPT-4 Turbo)
    - Multi-step reasoning (extract -> analyze -> synthesize)
    - More comprehensive context analysis
    - No time pressure - can use 1000+ token responses
    """
    
    def __init__(self, llm_client, llm_model: str):
        self.llm_client = llm_client
        self.llm_model = llm_model
    
    async def generate_conversation_summary(
        self,
        messages: List[Dict],
        user_id: str,
        bot_name: str
    ) -> Dict[str, Any]:
        """
        Generate comprehensive conversation summary
        
        Returns:
            {
                'summary_text': str,
                'key_topics': List[str],
                'emotional_tone': str,
                'compression_ratio': float,
                'confidence_score': float
            }
        """
        logger.debug(f"Generating summary for {len(messages)} messages...")
        
        # Build conversation context
        conversation_text = self._format_messages_for_llm(messages)
        
        # Generate summary with high-quality LLM
        summary_prompt = f"""You are an expert conversation analyst. Summarize this conversation between a user and {bot_name} (an AI character).

Focus on:
1. Key topics discussed
2. Important information shared
3. Emotional tone and evolution
4. Decisions made or plans discussed
5. Any personal details or preferences mentioned

Conversation ({len(messages)} messages):
{conversation_text}

Provide a comprehensive 3-5 sentence summary that captures the essence of this conversation. Be specific and preserve important details."""

        response = await self.llm_client.generate_completion(
            prompt=summary_prompt,
            model=self.llm_model,
            temperature=0.5,
            max_tokens=500
        )
        
        summary_text = response.strip()
        
        # Extract key topics
        key_topics = await self._extract_key_topics(conversation_text)
        
        # Analyze emotional tone
        emotional_tone = await self._analyze_emotional_tone(messages)
        
        # Calculate compression ratio
        original_length = sum(len(m['content']) for m in messages)
        compression_ratio = len(summary_text) / original_length if original_length > 0 else 0
        
        # Confidence score (simple heuristic - could be enhanced)
        confidence_score = 0.8 if len(messages) >= 10 else 0.6
        
        return {
            'summary_text': summary_text,
            'key_topics': key_topics,
            'emotional_tone': emotional_tone,
            'compression_ratio': compression_ratio,
            'confidence_score': confidence_score
        }
    
    def _format_messages_for_llm(self, messages: List[Dict]) -> str:
        """Format messages for LLM context"""
        formatted = []
        
        for msg in messages:
            role = "User" if msg['memory_type'] == 'user_message' else "Bot"
            content = msg['content'][:500]  # Truncate very long messages
            timestamp = msg.get('timestamp', '')
            
            formatted.append(f"[{timestamp}] {role}: {content}")
        
        return "\n\n".join(formatted)
    
    async def _extract_key_topics(self, conversation_text: str) -> List[str]:
        """Extract 3-5 key topics from conversation"""
        topics_prompt = f"""Extract the 3-5 main topics from this conversation. Return only a JSON array of topic strings.

Conversation:
{conversation_text[:2000]}

Topics (JSON array):"""

        response = await self.llm_client.generate_completion(
            prompt=topics_prompt,
            model=self.llm_model,
            temperature=0.3,
            max_tokens=100
        )
        
        # Parse JSON array
        try:
            import json
            topics = json.loads(response)
            return topics[:5]  # Max 5 topics
        except:
            # Fallback: split by commas
            return [t.strip() for t in response.split(',')[:5]]
    
    async def _analyze_emotional_tone(self, messages: List[Dict]) -> str:
        """Analyze overall emotional tone of conversation"""
        # Use existing emotion labels if available
        emotions = [m.get('emotion_label', 'neutral') for m in messages]
        
        # Simple majority vote
        emotion_counts = {}
        for emotion in emotions:
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        dominant_emotion = max(emotion_counts, key=emotion_counts.get)
        
        # Map to simple categories
        if dominant_emotion in ['joy', 'excitement', 'love']:
            return 'positive'
        elif dominant_emotion in ['sadness', 'anger', 'fear']:
            return 'negative'
        elif dominant_emotion in ['neutral', 'curiosity']:
            return 'neutral'
        else:
            return 'mixed'
```

---

### 4. Docker Configuration

```yaml
# docker-compose.multi-bot.template.yml (ADD THIS SERVICE)

services:
  # ... existing services ...
  
  # NEW: Background Enrichment Worker
  enrichment-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.enrichment-worker
    container_name: whisperengine_enrichment_worker
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=whisperengine_db
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      
      # Enrichment config
      - ENRICHMENT_INTERVAL_SECONDS=300  # 5 minutes
      - ENRICHMENT_BATCH_SIZE=50
      - LLM_MODEL=anthropic/claude-3.5-sonnet
      
      # API keys
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
    depends_on:
      - postgres
      - qdrant
    networks:
      - whisperengine-network
    restart: unless-stopped
    volumes:
      - ./logs/enrichment:/app/logs
```

```dockerfile
# docker/Dockerfile.enrichment-worker

FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements-enrichment.txt .
RUN pip install --no-cache-dir -r requirements-enrichment.txt

# Copy only necessary source code
COPY src/enrichment/ ./src/enrichment/
COPY src/llm/ ./src/llm/
COPY src/database/ ./src/database/

# Run enrichment worker
CMD ["python", "-m", "src.enrichment.worker"]
```

```txt
# requirements-enrichment.txt (minimal dependencies)

qdrant-client>=1.7.0
asyncpg>=0.29.0
openai>=1.0.0  # For OpenRouter API
```

---

## üîó MINIMAL BOT INTEGRATION (Phase 2)

### MessageProcessor Changes (MINIMAL, OPTIONAL)

```python
# src/core/message_processor.py

class MessageProcessor:
    
    async def _build_conversation_context_structured(
        self,
        user_id: str,
        message_content: str,
        character_name: str
    ) -> Dict[str, Any]:
        """
        Build conversation context with OPTIONAL pre-computed summaries
        
        NO BREAKING CHANGES - gracefully falls back if summaries don't exist
        """
        # Existing behavior: Retrieve recent conversation from Qdrant
        recent_memories = await self.memory_manager.retrieve_relevant_memories(
            user_id=user_id,
            query=message_content,
            limit=10
        )
        
        # NEW: Try to retrieve conversation summaries from PostgreSQL
        conversation_summaries = []
        if hasattr(self, 'db_pool') and self.db_pool:
            try:
                conversation_summaries = await self._retrieve_conversation_summaries(
                    user_id=user_id,
                    bot_name=character_name,
                    time_range_days=30
                )
            except Exception as e:
                # Graceful degradation - log and continue without summaries
                logger.debug(f"Could not retrieve summaries (enrichment may not be active): {e}")
        
        # Build context - summaries are OPTIONAL enhancement
        context = {
            "recent_conversation": self._format_memories(recent_memories),
            "current_message": message_content
        }
        
        # Add summaries if available
        if conversation_summaries:
            context["conversation_history"] = self._format_summaries(conversation_summaries)
            logger.info(f"‚úÖ Using {len(conversation_summaries)} pre-computed summaries for context")
        
        return context
    
    async def _retrieve_conversation_summaries(
        self,
        user_id: str,
        bot_name: str,
        time_range_days: int = 30
    ) -> List[Dict]:
        """
        Retrieve conversation summaries from PostgreSQL
        
        Returns empty list if table doesn't exist or no summaries found.
        """
        from datetime import datetime, timedelta
        
        cutoff_date = datetime.utcnow() - timedelta(days=time_range_days)
        
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT 
                    summary_text,
                    start_timestamp,
                    end_timestamp,
                    message_count,
                    key_topics,
                    emotional_tone
                FROM conversation_summaries
                WHERE user_id = $1 
                  AND bot_name = $2
                  AND end_timestamp >= $3
                ORDER BY end_timestamp DESC
                LIMIT 10
            """, user_id, bot_name, cutoff_date)
        
        return [dict(row) for row in rows]
    
    def _format_summaries(self, summaries: List[Dict]) -> str:
        """Format conversation summaries for LLM context"""
        formatted = []
        
        for summary in summaries:
            start = summary['start_timestamp'].strftime('%B %d')
            end = summary['end_timestamp'].strftime('%B %d')
            text = summary['summary_text']
            topics = ', '.join(summary.get('key_topics', [])[:3])
            
            formatted.append(f"[{start} - {end}] ({topics}): {text}")
        
        return "\n\n".join(formatted)
```

**Key Points**:
- ‚úÖ **Zero breaking changes** - bots work without summaries
- ‚úÖ **Graceful degradation** - try/except wrapper for summary retrieval
- ‚úÖ **Optional enhancement** - summaries added IF they exist
- ‚úÖ **Backwards compatible** - works with or without enrichment worker running

---

## üöÄ DEPLOYMENT PLAN

### Phase 1: Deploy Enrichment Worker (Week 1)

**Steps**:
1. ‚úÖ Create PostgreSQL migration for `conversation_summaries` table
2. ‚úÖ Implement enrichment worker code
3. ‚úÖ Create Docker container for worker
4. ‚úÖ Add worker to `docker-compose.multi-bot.yml`
5. ‚úÖ Deploy worker container - **NO BOT CHANGES NEEDED**
6. ‚úÖ Monitor worker logs - watch summaries being generated

**Expected Result**: Enrichment worker runs in background, starts populating `conversation_summaries` table with historical data.

**Risk**: ZERO - bots unchanged, worker operates independently

---

### Phase 2: Enable Summary Usage (Week 2)

**Steps**:
1. ‚úÖ Add minimal `_retrieve_conversation_summaries()` method to MessageProcessor
2. ‚úÖ Update `_build_conversation_context_structured()` to use summaries if available
3. ‚úÖ Test with one bot (Jake - minimal complexity)
4. ‚úÖ Monitor LLM context quality and response improvements
5. ‚úÖ Roll out to all bots

**Expected Result**: Bots start using pre-computed summaries for richer context. Users can ask "what did we talk about last week?" and get intelligent responses.

**Risk**: VERY LOW - graceful fallback if summaries don't exist

---

### Phase 3: Monitor & Iterate (Ongoing)

**Metrics to Track**:
- **Enrichment throughput**: Messages processed per minute
- **Summary coverage**: % of conversations with summaries
- **Summary quality**: User feedback on time-based queries
- **Cost**: LLM API costs for enrichment vs hot path
- **Worker uptime**: % time enrichment worker is healthy

**Optimizations**:
- Tune enrichment interval (5 min ‚Üí 10 min if cost is high)
- Adjust time window size (24h ‚Üí 12h for more granular summaries)
- Enhance LLM prompts for better summary quality
- Add semantic search on summaries for better retrieval

---

## üìä COMPARISON: CURRENT vs PROPOSED

| Aspect | Current System | With Async Enrichment |
|--------|---------------|----------------------|
| **Hot Path Latency** | 3,988ms | ~3,988ms (NO CHANGE initially) |
| **Conversation Context** | Recent 10 messages | Recent messages + timestamped summaries |
| **Time-Based Queries** | ‚ùå Not supported | ‚úÖ "What did we talk about last week?" |
| **Fact Extraction** | ‚úÖ Already in PostgreSQL (user_facts) | ‚úÖ Keep existing system |
| **Summary Quality** | N/A (not using summaries) | High-quality (Claude 3.5 Sonnet) |
| **Cost Impact** | Current LLM costs | +$0.01-0.02 per summary (async, batched) |
| **Bot Code Changes** | N/A | MINIMAL (10-20 lines, optional) |
| **Deployment Risk** | N/A | ZERO (separate container) |

---

## üéØ SUCCESS CRITERIA

### Phase 1 Success (Enrichment Worker)
- ‚úÖ Enrichment worker runs continuously without crashes
- ‚úÖ Summaries being generated for all active users
- ‚úÖ PostgreSQL `conversation_summaries` table populating
- ‚úÖ No impact on bot performance

### Phase 2 Success (Bot Integration)
- ‚úÖ Bots successfully retrieve and use summaries
- ‚úÖ Time-based queries ("last week", "yesterday") work correctly
- ‚úÖ No regressions in bot response quality
- ‚úÖ Graceful fallback when summaries not available

### Phase 3 Success (Optimization)
- ‚úÖ 80%+ of conversations have summaries within 24 hours
- ‚úÖ User satisfaction with time-based conversation recall
- ‚úÖ Cost increase <10% of current LLM spending
- ‚úÖ Summary quality rating >4/5 from user feedback

---

## üö® RISKS & MITIGATION

### Risk 1: Enrichment Worker Crashes
**Impact**: Summaries stop being generated (bots still work normally)  
**Mitigation**: Docker restart policy + health checks + monitoring alerts

### Risk 2: Summary Quality Issues
**Impact**: Summaries are inaccurate or not useful  
**Mitigation**: Iterative prompt tuning + user feedback loop + confidence scoring

### Risk 3: PostgreSQL Query Load
**Impact**: Summary retrieval slows down message processing  
**Mitigation**: Proper indexing + query optimization + request-scoped caching

### Risk 4: Cost Overrun
**Impact**: Enrichment LLM calls too expensive  
**Mitigation**: Batch processing + selective enrichment + cost monitoring + cheaper models

---

## üìö NEXT STEPS

### Immediate Actions
1. ‚úÖ **User approval** for incremental add-on approach
2. ‚úÖ Create PostgreSQL migration for `conversation_summaries`
3. ‚úÖ Implement enrichment worker MVP
4. ‚úÖ Deploy worker container (Phase 1)

### Short-term (1-2 weeks)
1. Monitor enrichment worker performance
2. Validate summary quality with sample data
3. Implement minimal bot integration (Phase 2)
4. Test time-based queries with real users

### Medium-term (1 month)
1. Full production deployment across all bots
2. Optimize enrichment prompts based on feedback
3. Add summary semantic search for better retrieval
4. Implement cost optimization strategies

---

**Last Updated**: October 19, 2025  
**Owner**: WhisperEngine Development Team  
**Status**: üü¢ Ready for implementation - awaiting approval
